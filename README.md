## MirScribe Pipeline - VCF Analysis Module

This repository contains the streamlined version of MirScribe pipeline's VCF analysis branch, focusing on pre-processing steps before XGBoost modeling.

### Prerequisites

- Python 3.8+
- Poetry (package manager)
- Git

### Installation

1. Install Poetry if not already installed:
```bash
pipx install poetry
```

2. Clone the repository:
```bash
git clone [repository-url]
cd [repository-name]
```

3. Install dependencies:
```bash
poetry install
```

4. Install Pyensembl data:
```bash
poetry run pyensembl install --release 75
```

The pipeline is now ready to use. See [Pipeline Architecture](#pipeline-architecture) section for details about the components and data flow.

## Pipeline Architecture

The pipeline follows a parallel processing architecture with retry mechanisms and streaming data handling.

### Function Call Graph

```
ğŸš€ run_pipeline
â”‚
â”œâ”€â”€â¡ï¸ yield_chunks
â”‚   â””â”€â”€ğŸ“Š pd.read_csv
â”‚
â”œâ”€â”€ğŸ”„ ThreadPoolExecutor
â”‚   â””â”€â”€ğŸ“¦ submit_chunk (nested function)
â”‚      â””â”€â”€ğŸ” process_chunk_with_retry
â”‚         â””â”€â”€âš™ï¸ process_chunk
â”‚            â””â”€â”€ğŸ› ï¸ transform_chunk
â”‚
â””â”€â”€âœ… as_completed
```

### Component Roles
- ğŸš€ run_pipeline: Main orchestrator
- â¡ï¸ yield_chunks: Data stream generator
- ğŸ”„ ThreadPoolExecutor: Parallel processing manager
- ğŸ” process_chunk_with_retry: Retry wrapper
- âš™ï¸ process_chunk: Core processing logic
- ğŸ› ï¸ transform_chunk: Data transformation

### Processing Flow
1. run_pipeline initiates the process
2. yield_chunks streams data
3. ThreadPoolExecutor manages parallel processing
4. Each chunk goes through retry wrapper
5. Process and transform functions handle the data
6. Results are collected via as_completed